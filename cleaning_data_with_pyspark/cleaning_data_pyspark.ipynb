{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data with PySpark\n",
    "\n",
    "Working with data is tricky - working with millions or even billions of rows is worse. Did you receive some data processing code written on a laptop with fairly pristine data? Chances are you’ve probably been put in charge of moving a basic data process from prototype to production. You may have worked with real world datasets, with missing fields, bizarre formatting, and orders of magnitude more data. Even if this is all new to you, this notebook helps you learn what’s needed to prepare data processes using Python with Apache Spark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a schema\n",
    "\n",
    "- Creating a defined schema helps with data quality and import performance. As mentioned during the lesson, we'll create a simple schema to read in the following columns:\n",
    "\n",
    "Name\n",
    "Age\n",
    "City\n",
    "The `Name` and `City` columns are `StringType()` and the `Age` column is an `IntegerType()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark.sql.types library ->Import * from the pyspark.sql.types library.\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define a new schema using the StructType method\n",
    "people_schema = StructType([\n",
    "  # Define a StructField for each field -> Define a StructField for name, age, and city. Each field should correspond to the correct datatype and not be nullable.\n",
    "  StructField('name', StringType(), False),\n",
    "  StructField('age', IntegerType(), False),\n",
    "  StructField('city', StringType(), False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/20 11:00:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ssamil-mbp-5.fritz.box:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to build a new SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark= SparkSession.builder.getOrCreate()\n",
    "    \n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the saved table in spark\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using lazy processing\n",
    "\n",
    "- Lazy processing operations will usually return in about the same amount of time regardless of the actual quantity of data. Remember that this is due to Spark not performing any transformations until an action is requested.\n",
    "\n",
    "- For this exercise, we'll be defining a Data Frame (aa_dfw_df) and add a couple transformations. Note the amount of time required for the transformations to complete when defined vs when the data is actually queried. These differences may be short, but they will be noticeable. When working with a full Spark cluster with larger quantities of data the difference will be more apparen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Actual elapsed time (Minutes)|airport|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|       01/01/2014|         0005|                          519|    hnl|\n",
      "|       01/01/2014|         0007|                          505|    ogg|\n",
      "|       01/01/2014|         0035|                          174|    slc|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "from pyspark.sql.functions import lower\n",
    "aa_dfw_df = spark.read.csv('/Users/ssamilozkan/Desktop/pyspark/cleaning_data_with_pyspark/dataset/AA_DFW_2014_Departures_Short.csv', header=True)\n",
    "\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', lower(aa_dfw_df['Destination Airport']))\n",
    "\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
    "\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a DataFrame in Parquet format\n",
    "\n",
    "- When working with Spark, you'll often start with CSV, JSON, or other data sources. This provides a lot of flexibility for the types of data to load, but it is not an optimal format for Spark. The Parquet format is a columnar data store, allowing Spark to use predicate pushdown. This means Spark will only process the data necessary to complete the operations you define versus reading the entire dataset. This gives Spark more flexibility in accessing the data and often drastically improves performance on large datasets.\n",
    "\n",
    "- In this exercise, we're going to practice creating a new Parquet file and then process some data from it.\n",
    "\n",
    "- The spark object and the df1 and df2 DataFrames have been setup for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv('/Users/ssamilozkan/Desktop/pyspark/cleaning_data_with_pyspark/dataset/AA_DFW_2014_Departures_Short.csv', header=True)\n",
    "df2 = spark.read.csv('/Users/ssamilozkan/Desktop/pyspark/cleaning_data_with_pyspark/dataset/AA_DFW_2015_Departures_Short.csv', header=True)\n",
    "print(type(df1))\n",
    "print(type(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 Count: 157198\n",
      "df2 Count: 146558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303756\n"
     ]
    }
   ],
   "source": [
    "# View the row count of df1 and df2\n",
    "print(\"df1 Count: %d\" % df1.count())\n",
    "print(\"df2 Count: %d\" % df2.count())\n",
    "\n",
    "# Combine the DataFrames into one\n",
    "df3 = df1.union(df2)\n",
    "\n",
    "# Save the df3 DataFrame in Parquet format\n",
    "df3.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')\n",
    "\n",
    "# Read the Parquet file into a new DataFrame and run a count\n",
    "print(spark.read.parquet('AA_DFW_ALL.parquet').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL and Parquet\n",
    "\n",
    "- Parquet files are perfect as a backing data store for SQL queries in Spark. While it is possible to run the same queries directly via Spark's Python functions, sometimes it's easier to run SQL queries alongside the Python options.\n",
    "\n",
    "- For this example, we're going to read in the Parquet file we created in the last exercise and register it as a SQL table. Once registered, we'll run a quick query against the table (aka, the Parquet file).\n",
    "\n",
    "- The spark object and the AA_DFW_ALL.parquet file are available for you automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the saved table in spark\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|       01/01/2015|         0005|                HNL|                          526|\n",
      "|       01/01/2015|         0007|                OGG|                          517|\n",
      "|       01/01/2015|         0023|                SFO|                          233|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the Parquet file into flights_df\n",
    "flights_df = spark.read.parquet('AA_DFW_ALL.parquet')\n",
    "flights_df.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+---------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|flight_duration|\n",
      "+-----------------+-------------+-------------------+---------------+\n",
      "|       01/01/2015|         0005|                HNL|            526|\n",
      "|       01/01/2015|         0007|                OGG|            517|\n",
      "|       01/01/2015|         0023|                SFO|            233|\n",
      "+-----------------+-------------+-------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df= flights_df.withColumnRenamed(\"Actual elapsed time (Minutes)\", \"flight_duration\")\n",
    "flights_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+---------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|flight_duration|\n",
      "+-----------------+-------------+-------------------+---------------+\n",
      "|       01/01/2015|         0125|                ATL|            103|\n",
      "|       01/01/2015|         0194|                ATL|              0|\n",
      "|       01/01/2015|         0232|                ATL|            113|\n",
      "+-----------------+-------------+-------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter rows where name starts with 'A'\n",
    "flights_df.filter(flights_df[\"Destination Airport\"].startswith('A')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='flights', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register the temp table\n",
    "flights_df.createOrReplaceTempView('flights')\n",
    "\n",
    "# check the saved table in spark\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+---------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|flight_duration|\n",
      "+-----------------+-------------+-------------------+---------------+\n",
      "|       01/01/2015|         0005|                HNL|            526|\n",
      "|       01/01/2015|         0007|                OGG|            517|\n",
      "|       01/01/2015|         0023|                SFO|            233|\n",
      "+-----------------+-------------+-------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "type(flights_df)\n",
    "flights_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): string (nullable = true)\n",
      " |-- Flight Number: string (nullable = true)\n",
      " |-- Destination Airport: string (nullable = true)\n",
      " |-- flight_duration: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average flight time is: 143\n"
     ]
    }
   ],
   "source": [
    "# Run a SQL query of the average flight duration\n",
    "avg_duration = spark.sql('SELECT avg(flight_duration) from flights').collect()[0]\n",
    "print('The average flight time is: %d' % avg_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Column Operations\n",
    "\n",
    "\n",
    "#### Filtering column content with Python\n",
    "You've looked at using various operations on DataFrame columns - now you can modify a real dataset. The DataFrame `voter_df` contains information regarding the voters on the Dallas City Council from the past few years. This truncated DataFrame contains the date of the vote being cast and the name and position of the voter. Your manager has asked you to clean this data so it can later be integrated into some desired reports. The primary task is to remove any null entries or odd characters and return a specific set of voters where you can validate their information.\n",
    "\n",
    "This is often one of the first steps in data cleaning - removing anything that is obviously outside the format. For this dataset, make sure to look at the original data and see what looks out of place for the `VOTER_NAME` column.\n",
    "\n",
    "The `pyspark.sql.functions` library is already imported under the alias F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|\n",
      "+----------+-------------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|\n",
      "+----------+-------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df = spark.read.csv('/Users/ssamilozkan/Desktop/pyspark/cleaning_data_with_pyspark/dataset/DallasCouncilVoters.csv', header=True)\n",
    "voter_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|VOTER_NAME                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Tennell Atkins                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|  the  final   2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for   the   apportionment   of   costs and the methods of assessing special assessments for the services and improvements to property in the District;  closing  the  hearing  and  levying  a  special  assessment  on  property  in  the  District              |\n",
      "|Scott Griggs                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Scott  Griggs                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|Sandy Greyson                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|Michael S. Rawlings                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "| the final 2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for  the   apportionment   of   costs and  the  methods  of  assessing  special  assessments  on  Dallas  hotels  with    100 or more rooms                                                                                                                           |\n",
      "|Kevin Felder                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Adam Medrano                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Casey  Thomas                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|null                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|   the   final  2018 Assessment  Plan  and  the  2018 Assessment  Roll  (to  be  kept  on  file   with the City Secretary); establishing classifications  for  the  apportionment  of  costs  and  the  methods  of  assessing  special  assessments  for  the  services  and  improvements  to  property  in  the  District;  closing  the  hearing  and  levying  a special  assessment  on  property  in  the  District |\n",
      "|011018__42                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|Mark  Clayton                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|Casey Thomas                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Sandy  Greyson                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|Mark Clayton                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Jennifer S.  Gates                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|Tiffinni A. Young                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|  the  final  2018 Assessment  Plan   and   the   2018 Assessment   Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing   classifications  for the apportionment of costs and the  methods  of  assessing  special  assessments for the services and improvements  to  property  in  the  District;  closing the hearing and  levying  a  special  assessment  on  property  in  the  District       |\n",
      "|B. Adam  McGough                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|Omar Narvaez                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|Philip T. Kingston                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|Rickey D. Callahan                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "|Dwaine R. Caraway                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|Philip T.  Kingston                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|Jennifer S. Gates                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|Lee M. Kleinman                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|Monica R. Alonzo                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|   the   final  2018 Assessment  Plan  and  the  2018 Assessment  Roll   (to  be  kept  on  file   with the City Secretary); establishing classifications  for  the  apportionment  of  costs  and  the  methods  of  assessing  special  assessments  for  the  services  and  improvements  to  property  in  the  District;  closing  the  hearing  and  levying  a special  assessment  on  property  in  the  District|\n",
      "|Rickey D.  Callahan                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|Carolyn King Arnold                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|  the  final   2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for  the   apportionment   of   costs and the methods of assessing special assessments for the services and improvements to property in the District;  closing  the  hearing  and  levying  a  special  assessment  on  property  in  the  District               |\n",
      "|Erik Wilson                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|  the  final  2018 Assessment Plan and the 2018 Assessment  Roll  (to  be  kept  on  file  with  the  City  Secretary);  establishing  classifications  for  the   apportionment   of   costs and the methods of assessing special assessments for the services and improvements to property in the District; closing  the  hearing  and  levying  a  special  assessment  on  property  in  the  District                 |\n",
      "|Lee Kleinman                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the distinct VOTER_NAME entries\n",
    "distinct_name_voter = voter_df.select(\"VOTER_NAME\").distinct().show(40, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+\n",
      "|DATE|TITLE|VOTER_NAME|\n",
      "+----+-----+----------+\n",
      "+----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter voter_df where the VOTER_NAME is 1-20 characters in length\n",
    "voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')\n",
    "voter_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|VOTER_NAME|\n",
      "+----------+\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter out voter_df where the VOTER_NAME contains an underscore\n",
    "voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))\n",
    "\n",
    "# Show the distinct VOTER_NAME entries again\n",
    "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying DataFrame columns\n",
    "\n",
    "- Previously, you filtered out any rows that didn't conform to something generally resembling a name. Now based on your earlier work, your manager has asked you to create two new columns - `first_name` and last_name. She asks you to split the `VOTER_NAME` column into words on any space character. You'll treat the last word as the `last_name`, and all other words as the `first_name`. You'll be using some new functions in this exercise including `.split()`, `.size()`, and `.getItem()`. The `.getItem(index)` takes an integer value to return the appropriately numbered item in the column. The functions `.split()` and `.size()` are in the `pyspark.sql.functions` library.\n",
    "\n",
    "- Please note that these operations are always somewhat specific to the use case. Having your data conform to a format often matters more than the specific details of the format. Rarely is a data cleaning task meant just for one person - matching a defined format allows for easier sharing of the data later (ie, Paul doesn't need to worry about names - Mary already cleaned the dataset).\n",
    "\n",
    "- The filtered voter DataFrame from your previous exercise is available as voter_df. The `pyspark.sql.functions` library is available under the alias F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|\n",
      "+----------+-------------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|\n",
      "+----------+-------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we have to call again csv file to some manipulations\n",
    "voter_df = spark.read.csv('/Users/ssamilozkan/Desktop/pyspark/cleaning_data_with_pyspark/dataset/DallasCouncilVoters.csv', header=True)\n",
    "voter_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|               split|\n",
      "+----------+-------------+-------------------+--------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|\n",
      "+----------+-------------+-------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn('split', F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "voter_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------------------+----------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|               split|first_name|\n",
      "+----------+-------------+-------------------+--------------------+----------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael|\n",
      "+----------+-------------+-------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn('first_name', voter_df.split.getItem(0))\n",
    "voter_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------------------+----------+---------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|               split|first_name|last_name|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssamilozkan/Desktop/pyspark/pvenv/lib/python3.10/site-packages/pyspark/sql/column.py:419: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn('last_name', voter_df.split.getItem(F.size('split')-1))\n",
    "voter_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+-------------------+----------+---------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|         random_val|first_name|last_name|\n",
      "+----------+-------------+-------------------+-------------------+----------+---------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates| 0.7301764769841849|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston| 0.7543098348614885|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|               null|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|0.45761288875457473|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas| 0.4761696744443019|     Casey|   Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|   0.88596071846336|   Carolyn|   Arnold|\n",
      "|02/08/2017|Councilmember|       Scott Griggs| 0.1450245707549751|     Scott|   Griggs|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|0.47760497438660254|        B.|  McGough|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|  0.433555477048288|       Lee| Kleinman|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson| 0.7379305330046815|     Sandy|  Greyson|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates| 0.5987421594792524|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston| 0.8406955195978493|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|               null|   Michael| Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano| 0.3607611143531271|      Adam|  Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas| 0.3667935803247886|     Casey|   Thomas|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold| 0.7140674710216044|   Carolyn|   Arnold|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan|  0.977985138374448|    Rickey| Callahan|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates| 0.7059531013209034|  Jennifer|    Gates|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson| 0.4462717984727965|     Sandy|  Greyson|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates| 0.4763935956960439|  Jennifer|    Gates|\n",
      "+----------+-------------+-------------------+-------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop the splits column\n",
    "voter_df = voter_df.drop('split')\n",
    "\n",
    "# Show the voter_df DataFrame\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional DataFrame column operations\n",
    "\n",
    "\n",
    "#### when() example\n",
    "- The `when()` clause lets you conditionally modify a Data Frame based on its content. You'll want to modify our `voter_df` DataFrame to add a random number to any voting member that is defined as a \"Councilmember\".\n",
    "\n",
    "- The `voter_df` DataFrame is defined and available to you. The `pyspark.sql.functions` library is available as `F.` You can use `F.rand()` to generate the random value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|\n",
      "+----------+-------------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|\n",
      "+----------+-------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv('/Users/ssamilozkan/Desktop/pyspark/cleaning_data_with_pyspark/dataset/DallasCouncilVoters.csv', header=True)\n",
    "voter_df = data.select('*') # to copy the dataframe in pyspark\n",
    "voter_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------------------+------------------+\n",
      "|      DATE|        TITLE|        VOTER_NAME|        random_val|\n",
      "+----------+-------------+------------------+------------------+\n",
      "|02/08/2017|Councilmember| Jennifer S. Gates|0.7301764769841849|\n",
      "|02/08/2017|Councilmember|Philip T. Kingston|0.7543098348614885|\n",
      "+----------+-------------+------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column to voter_df for any voter with the title 'Councilmember'\n",
    "voter_df = voter_df.withColumn('random_val', F.when(voter_df['TITLE'] == 'Councilmember', F.rand()))\n",
    "\n",
    "# Show some of the DataFrame rows, noting whether the when clause worked\n",
    "voter_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When / Otherwise\n",
    "\n",
    "- This requirement is similar to the last, but now you want to add multiple values based on the voter's position. Modify your `voter_df` DataFrame to add a random number to any voting member that is defined as a `Councilmember`. Use 2 for the `Mayor` and 0 for anything other position.\n",
    "\n",
    "- The `voter_df` Data Frame is defined and available to you. The `pyspark.sql.functions` library is available as `F.` You can use `F.rand()` to generate the random value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|\n",
      "+----------+-------------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|\n",
      "+----------+-------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df1 = data.select('*') # to copy the dataframe in pyspark\n",
    "voter_df1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|        random_val|\n",
      "+----------+-------------+-------------------+------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|0.5489358770778864|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|0.9943753680960055|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|               2.0|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|0.4595390703545188|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|0.7497170038838954|\n",
      "+----------+-------------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column to voter_df named random_val with the results of the F.rand() method for any voter with the title Councilmember.\n",
    "# Set random_val to 2 for the Mayor. Set any other title to the value 0.\n",
    "voter_df1 = voter_df1.withColumn('random_val', \n",
    "                                 F.when(voter_df1['TITLE'] == 'Councilmember', F.rand())\n",
    "                                 .when(voter_df1['TITLE'] == 'Mayor', 2).otherwise(0))\n",
    "voter_df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------------+----------+\n",
      "|      DATE|               TITLE|       VOTER_NAME|random_val|\n",
      "+----------+--------------------+-----------------+----------+\n",
      "|04/25/2018|Deputy Mayor Pro Tem|     Adam Medrano|       0.0|\n",
      "|04/25/2018|       Mayor Pro Tem|Dwaine R. Caraway|       0.0|\n",
      "|06/20/2018|Deputy Mayor Pro Tem|     Adam Medrano|       0.0|\n",
      "+----------+--------------------+-----------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the .filter clause to find 0 in random_val.\n",
    "# Use the .filter() clause with random_val\n",
    "voter_df1.filter(voter_df1.random_val== 0).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User defined functions\n",
    "\n",
    "#### Using user defined functions in Spark\n",
    "- You've seen some of the power behind Spark's built-in string functions when it comes to manipulating DataFrames. However, once you reach a certain point, it becomes difficult to process the data in a without creating a rat's nest of function calls. Here's one place where you can use User Defined Functions to manipulate our DataFrames.\n",
    "\n",
    "- For this exercise, we'll use our `voter_df` DataFrame, but you're going to replace the `first_name` column with the first and middle names.\n",
    "\n",
    "- The `pyspark.sql`.functions library is available under the alias F. The classes from pyspark.sql.types are already imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------------------+----------+---------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|               split|first_name|last_name|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voter_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit the getFirstAndMiddle() function to return a space separated string of names, except the last entry in the names list.\n",
    "def getFirstAndMiddle(names):\n",
    "  # Return a space separated string of names\n",
    "  return ' '.join(names[0:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- TITLE: string (nullable = true)\n",
      " |-- VOTER_NAME: string (nullable = true)\n",
      " |-- split: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- first_and_middle_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the method as a UDF\n",
    "from pyspark.sql.types import StringType\n",
    "udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())\n",
    "\n",
    "# Create a new column using your UDF\n",
    "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.split))\n",
    "\n",
    "voter_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an ID Field\n",
    "\n",
    "- When working with data, you sometimes only want to access certain fields and perform various operations. In this case, find all the unique voter names from the DataFrame and add a unique ID number. Remember that Spark IDs are assigned based on the DataFrame partition - as such the ID values may be much greater than the actual number of rows in the DataFrame.\n",
    "\n",
    "- With Spark's lazy processing, the IDs are not actually generated until an action is performed and can be somewhat random depending on the size of the dataset.\n",
    "\n",
    "- The spark session and a Spark DataFrame df containing the DallasCouncilVotes.csv.gz file are available in your workspace. The pyspark.sql.functions library is available under the alias F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+--------+-------------+-------------------+---------+------------------+-----------------------+------------------+--------------------+\n",
      "|      DATE|AGENDA_ITEM_NUMBER|ITEM_TYPE|DISTRICT|        TITLE|         VOTER NAME|VOTE CAST|FINAL ACTION TAKEN|AGENDA ITEM DESCRIPTION|         AGENDA_ID|             VOTE_ID|\n",
      "+----------+------------------+---------+--------+-------------+-------------------+---------+------------------+-----------------------+------------------+--------------------+\n",
      "|02/08/2017|                 1|   AGENDA|      13|Councilmember|  Jennifer S. Gates|      N/A|  NO ACTION NEEDED|          Call to Order|020817__Special__1|020817__Special__...|\n",
      "|02/08/2017|                 1|   AGENDA|      14|Councilmember| Philip T. Kingston|      N/A|  NO ACTION NEEDED|          Call to Order|020817__Special__1|020817__Special__...|\n",
      "|02/08/2017|                 1|   AGENDA|      15|        Mayor|Michael S. Rawlings|      N/A|  NO ACTION NEEDED|          Call to Order|020817__Special__1|020817__Special__...|\n",
      "+----------+------------------+---------+--------+-------------+-------------------+---------+------------------+-----------------------+------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('/Users/ssamilozkan/Desktop/pyspark/cleaning_data_with_pyspark/dataset/2DallasCouncilVotes.csv', header=True)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|    VOTER NAME|\n",
      "+--------------+\n",
      "|Tennell Atkins|\n",
      "|  Scott Griggs|\n",
      "| Scott  Griggs|\n",
      "+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select all the unique council voters\n",
    "voter_df = df.select(df[\"VOTER NAME\"]).distinct()\n",
    "voter_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 36 rows in the voter_df DataFrame.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count the rows in voter_df\n",
    "print(\"\\nThere are %d rows in the voter_df DataFrame.\\n\" % voter_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|    VOTER NAME|ROW_ID|\n",
      "+--------------+------+\n",
      "|Tennell Atkins|     0|\n",
      "|  Scott Griggs|     1|\n",
      "| Scott  Griggs|     2|\n",
      "+--------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a ROW_ID\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "voter_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|         VOTER NAME|ROW_ID|\n",
      "+-------------------+------+\n",
      "|     Tennell Atkins|     0|\n",
      "|       Scott Griggs|     1|\n",
      "|      Scott  Griggs|     2|\n",
      "|      Sandy Greyson|     3|\n",
      "|Michael S. Rawlings|     4|\n",
      "|       Kevin Felder|     5|\n",
      "|       Adam Medrano|     6|\n",
      "|               null|     7|\n",
      "|   Casey Thomas, II|     8|\n",
      "|      Mark  Clayton|     9|\n",
      "+-------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the rows with 10 min IDs in the set\n",
    "voter_df.orderBy(voter_df.ROW_ID.asc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDs with different partitions\n",
    "\n",
    "- You've just completed adding an ID field to a DataFrame. Now, take a look at what happens when you do the same thing on DataFrames containing a different number of partitions.\n",
    "\n",
    "- To check the number of partitions, use the method .rdd.getNumPartitions() on a DataFrame.\n",
    "\n",
    "- The spark session and two DataFrames, voter_df and voter_df_single, are available in your workspace. The instructions will help you discover the difference between the DataFrames. The pyspark.sql.functions library is available under the alias F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 1 partitions in the voter_df DataFrame.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the number of partitions in each DataFrame\n",
    "print(\"\\nThere are %d partitions in the voter_df DataFrame.\\n\" % voter_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a ROW_ID field to each DataFrame\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|          VOTER NAME|ROW_ID|\n",
      "+--------------------+------+\n",
      "|  the  final  201...|    35|\n",
      "|  the  final   20...|    34|\n",
      "|   the   final  2...|    33|\n",
      "|  the  final  201...|    32|\n",
      "|   the   final  2...|    31|\n",
      "| the final 2018 A...|    30|\n",
      "|  the  final   20...|    29|\n",
      "|          011018__42|    28|\n",
      "|        Lee Kleinman|    27|\n",
      "|         Erik Wilson|    26|\n",
      "+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Show the top 10 IDs in each DataFrame \n",
    "print(voter_df.orderBy(voter_df.ROW_ID.desc()).show(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching a DataFrame\n",
    "\n",
    "- You've been assigned a task that requires running several analysis operations on a DataFrame. You've learned that caching can improve performance when reusing DataFrames and would like to implement it.\n",
    "\n",
    "- You'll be working with a new dataset consisting of airline departure information. It may have repetitive data and will need to be de-duplicated.\n",
    "\n",
    "- The DataFrame departures_df is defined, but no actions have been performed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "|       01/01/2016|            5|                HNL|                          529|\n",
      "|       01/01/2016|            7|                OGG|                          512|\n",
      "|       01/01/2016|           25|                PHL|                          161|\n",
      "+-----------------+-------------+-------------------+-----------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departures_df = spark.read.csv('/Users/ssamilozkan/Desktop/pyspark/cleaning_data_with_pyspark/dataset/AA_DFW_2016_Departures_Short.csv', header=True, inferSchema=True)\n",
    "departures_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting 140604 rows took 3.456379 seconds\n",
      "Counting 140604 rows again took 0.868809 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start_time = time.time()\n",
    "\n",
    "# Add caching to the unique rows in departures_df\n",
    "departures_df = departures_df.distinct().cache()\n",
    "\n",
    "# Count the unique rows in departures_df, noting how long the operation takes\n",
    "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
    "\n",
    "# Count the rows again, noting the variance in time of a cached DataFrame\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing a DataFrame from cache\n",
    "\n",
    "- You've finished the analysis tasks with the departures_df DataFrame, but have some other processing to do. You'd like to remove the DataFrame from the cache to prevent any excess memory usage on your cluster.\n",
    "\n",
    "- The DataFrame departures_df is defined and has already been cached for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is departures_df cached?: True\n",
      "Removing departures_df from cache\n",
      "Is departures_df cached?: False\n"
     ]
    }
   ],
   "source": [
    "# Determine if departures_df is in the cache\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
    "print(\"Removing departures_df from cache\")\n",
    "\n",
    "# Remove departures_df from the cache\n",
    "departures_df.unpersist()\n",
    "\n",
    "# Check the cache status again\n",
    "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File import performance\n",
    "\n",
    "- You've been given a large set of data to import into a Spark DataFrame. You'd like to test the difference in import speed by splitting up the file.\n",
    "\n",
    "- You have two types of files available: `departures_full.txt.gz` and `departures_xxx.txt.gz` where xxx is 000 - 013. The same number of rows is split between each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in full DataFrame:\t140604\n",
      "Time to run: 0.175895\n",
      "Total rows in split DataFrame:\t140604\n",
      "Time to run: 0.127455\n"
     ]
    }
   ],
   "source": [
    "# Import the full and split files into DataFrames\n",
    "full_df = spark.read.csv('/Users/ssamilozkan/Desktop/pyspark/cleaning_data_with_pyspark/dataset/AA_DFW_2016_Departures_Short.csv', header=True, inferSchema=True)\n",
    "split_df = spark.read.csv('/Users/ssamilozkan/Desktop/pyspark/cleaning_data_with_pyspark/dataset/AA_DFW_2016_*.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Print the count and run time for each DataFrame\n",
    "start_time_a = time.time()\n",
    "print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
    "\n",
    "start_time_b = time.time()\n",
    "print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pvenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "699a4b6e8fbb2484f9afd79cc742583a739fbcd250a0268074077a67114f0493"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
